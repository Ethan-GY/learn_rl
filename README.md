# 这是我学习强化学习的仓库。
# 目前完成了两个利用Q-learning算法实现的小demo。

## 1. demo_OT
实现了一个简单的一维世界中的强化学习示例，使用Q-learning表格方法。在这个世界中，智能体（agent）"o"位于左侧，目标宝藏（treasure）"T"位于最右侧位置。智能体需要学习如何找到宝藏的最佳策略。

  首先创建一个表格，行为state，列为action，初始时都设为0。

  接着实现了ε-贪婪策略：

有EPSILON（90%）的概率选择当前状态下Q值最大的动作（贪婪），

有1-EPSILON（10%）的概率随机选择动作（探索），

如果当前状态下所有动作的Q值都为0，也会随机选择动作

  然后定义环境的动态：

如果向右移动且到达终点前一格，则到达终点（'terminal'）并获得奖励1，

如果向右移动但未到达终点前一格，则向右移动一格，奖励为0，

如果向左移动且不在最左侧，则向左移动一格，奖励为0，

如果向左移动但已在最左侧，则保持原位（碰墙），奖励为0。

  再可视化当前环境状态：

创建一个环境列表，最右侧是宝藏'T'，其余位置是'-'，

如果智能体到达终点，显示当前回合和总步数，

否则，在智能体当前位置显示'o'，并刷新显示。

  最后是Q-learning算法的核心实现



## 2. demo_maze
  使用Q-learning算法解决迷宫问题的强化学习示例。整个项目分为三个主要文件：

1. maze_env.py - 迷宫环境的实现

  使用tkinter库创建了一个可视化界面，作出定义：

- 红色方块 ：探索者(agent)

- 黑色方块 ：陷阱(hell)，进入获得-1奖励

- 黄色圆形 ：目标(paradise)，进入获得+1奖励

- 其他区域 ：普通地面，奖励为0

2. RL_brain.py - Q-learning算法的实现

  实现了Q-learning算法的核心逻辑：设定了参数，定义choose_action贪婪策略方法

3. run_this.py - 主程序

  控制整个学习过程，循环训练100回合。
- 经过多次训练后，Q表会收敛到最优策略
- 智能体学会避开陷阱，找到到达目标的最短路径
